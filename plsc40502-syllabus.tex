\documentclass[11pt, article, oneside]{memoir}

%% Required packages
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx, url}
\usepackage{rotating}
\usepackage{multicol}
\usepackage[small,it]{caption}
\usepackage{subfig}
%\usepackage{fullpage, setspace}
\usepackage{epigraph}
\usepackage[all]{xy}
\usepackage{verbatim}
\usepackage[authordate, backend=biber, doi=false, isbn=false,
            backref=true, maxbibnames=10, hyperref=true,
            dateabbrev=false, uniquename=false]{biblatex-chicago}


%% For putting floats at the end
%\usepackage[nolists]{endfloat}


%% XeLaTeX packages + options
\usepackage{mathspec}
\usepackage{xunicode}
\defaultfontfeatures{Mapping=tex-text,Scale=MatchLowercase,Numbers=OldStyle}
%\setsansfont{Helvetica Neue Light}
%\setsansfont{Quicksand}
\setsansfont{Linux Biolinum O}
\setmainfont{Linux Libertine O}
\setmonofont{Inconsolata}
\setmathrm{Linux Libertine O}
\setmathsfont(Latin)[Uppercase=Italic, Lowercase=Italic,
Kerning=Off]{Linux Libertine O}
\setmathsfont(Greek)[Uppercase=Regular, Lowercase=Regular]{Linux Libertine O}
\setmathsfont(Digits)[Numbers={Lining,Proportional}]{Linux Libertine O}


%% Tikz for drawing figures
\usepackage{pgf, tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{arrows}


%% Fancy section labels
\usepackage[compact]{titlesec}
\titleformat{\section}[hang]{\Large\sffamily\bfseries}{\S{\addfontfeatures{Numbers=OldStyle}\thesection}}{1em}{}{}
\titleformat{\subsection}[hang]{\large\sffamily\bfseries}{\addfontfeatures{Numbers=OldStyle}\thesubsection}{1em}{}
\titlespacing*{\section}{0em}{1.5em}{0.5em}
\titlespacing*{\subsection}{0em}{1.5em}{0.5em}


\newcommand{\vs}{\vspace{-\baselineskip}}
\theoremstyle{Assumption}
\newtheorem{assump}{Assumption}
\newcommand{\indep}{\perp\!\!\!\perp}

%% Bibliography files
%\addbibresource{mb.bib}
%\addbibresource{gk.bib}
%\addbibresource{gkpubs.bib}

%% Put url links in titles of bibliography
\ExecuteBibliographyOptions{url=false}
\ExecuteBibliographyOptions{doi=false}
\newbibmacro{string+url}[1]{%
 \iffieldundef{doi}{\iffieldundef{url}{#1}{\href{\thefield{url}}{#1}}}{\href{http://dx.doi.org/\thefield{doi}}{#1}}}
\DeclareFieldFormat{title}{\usebibmacro{string+url}{\mkbibemph{#1}}}
\DeclareFieldFormat[article]{title}{\usebibmacro{string+url}{\mkbibquote{#1}}}
\DeclareFieldFormat[misc]{title}{\usebibmacro{string+url}{\mkbibemph{#1}}}
\DeclareFieldFormat[book]{title}{\usebibmacro{string+url}{\mkbibemph{#1}}}

%% Name, Title, Affiliation, Contact. Change as needed.
\def\myaffiliation{Department of Political Science, University of Chicago}
\def\myauthor{Anton Strezhnev}
\def\myemail{\texttt{\href{mailto:astrezhnev@uchicago.edu}{astrezhnev@uchicago.edu}}}
\def\mywebsite{\mbox{\url{http://www.antonstrezhnev.com}}}
\def\myaddress{Pick Hall 328, 3rd floor, 5828 S University Ave}
\def\mytitle{PLSC 40502: Data Analysis with Statistical Models}
\def\mykeywords{Anton Strezhnev, Statistical Models}


%% Custom colors
\definecolor{gray}{rgb}{0.459,0.438,0.471}
\definecolor{crimson}{rgb}{0.34, 0.18, 0.55}


%% Create a command to make a note at the top of the first page describing the
%% publication status of the paper. 
\newcommand{\published}[1]{% 
   \gdef\puB{#1}} 
   \newcommand{\puB}{} 
   \renewcommand{\maketitlehooka}{% 
       \par\noindent\small \puB} 

\usepackage[plainpages=false, 
            pdfpagelabels, 
            bookmarksnumbered,
            pdftitle={\mytitle}, 
            pdfauthor={\myauthor},
            pdfkeywords={\mykeywords},
            colorlinks=true,
            citecolor=crimson, 
            linkcolor=crimson, 
            urlcolor=crimson]{hyperref} 
% \makeatletter
% \newcommand\org@hypertarget{}
% \let\org@hypertarget\hypertarget
% \renewcommand\hypertarget[2]{%
% \Hy@raisedlink{\org@hypertarget{#1}{}}#2%
% } \makeatother


%% blank label items; hanging bibs for text
%% Custom hanging indent for vita items
\def\ind{\hangindent=1 true cm\hangafter=1 \noindent}
\def\labelitemi{$\cdot$}

    % Title flush left
    \pretitle{\begin{flushleft}\Huge\sffamily}
    \posttitle{\end{flushleft}\par\vskip 0.5em}
    \preauthor{\begin{flushleft}\sffamily  \Large \vspace{0.25em}}
    \postauthor{\end{flushleft}}
    \predate{\begin{flushleft}\sffamily \small\vspace{0.9em}}
    \postdate{\end{flushleft}\par\vskip 2em}

\title{{\mytitle}}

\author{\myauthor\smallskip\footnotesize\newline Office: Pick Hall 328, 3rd floor
  \newline Office Hours: Tuesdays 4pm-6pm or schedule an appointment by e-mail \newline
    \myemail \newline \mywebsite
\newline
}

\published{{\sffamily Winter 2025 | Lecture: Tues/Thurs.,  9:30am-10:50am, | Room: Harper Memorial 135 (Lecture) | Units: 100}}

\counterwithout{section}{chapter}

\date{}
\begin{document}
\maketitle
\textbf{Last Updated: 01/6/2025}
\section*{Course Overview}

Statistical models provide a structure for the analysis of data. Often many scientific questions revolve around drawing statistical inferences about some parameter, such as a regression coefficient. Models also allow researchers to generate predictions on new or out-of-sample data. Understanding the fundamentals of how to define, estimate and validate a statistical model is essential to the process of quantitative empirical research.

This course is part of the second year of the Quantitative Methodology sequence in the Department of Political Science and builds on the first year sequence (PLSC 30500, 30600, 30700). It will introduce students to likelihood and Bayesian inference with a focus on multilevel/hierarchical regression models. The overarching framework of this class is model-based inference for description and prediction -- a complement to the design-based framework of PLSC 30600 Causal Inference. Students will learn both the theory behind Bayesian modeling as well as how to implement common estimators (e.g. Expectation-Maximization, Markov Chain Monte Carlo (MCMC)) in the R statistical programming language. Applied examples will be drawn from across the political science literature, with a particular emphasis on the analysis of large survey data (e.g. the American National Election Survey (ANES), the Cooperative Election Survey (CES), the European Social Survey (ESS)).

This course will involve a combination of lectures and problem sets. Lectures will focus on introducing the core theoretical concepts being taught in this course as well as providing illustrations through worked applied examples. Problem sets will contain a mixture of both theoretical and applied questions and serve to reinforce key concepts and allow students to assess their progress and understanding throughout the course. Primary evaluation will take the form of a take-home midterm and final exam.

Assignments will involve analysis of data using the R programming language. This is a free and open source language for statistical computing that is used extensively for data analysis in many fields. Prior experience with the fundamentals of R programming is required.

\section*{Prerequisites}

This course assumes that you have both a background in the core concepts of probability, statistics and inference as well as prior exposure to linear regression models. Completing the first three courses in the political science graduate methodology sequence should prepare you for the material in this class. However, there are no strict, specific course pre-requisites as many different disciplines and departments offer introductory statistics classes that cover the relevant material.

If you are unsure of whether you meet the requirements, skim/read through the first six chapters of \textit{Regression and Other Stories}, one of the books being used by this course. You should find most of the concepts behind the material relatively familiar, aside from the references to Bayesian models (which will be covered in this course).

Please contact the instructor at (\href{mailto:astrezhnev@uchicago.edu}{astrezhnev@uchicago.edu}) if you are interested in enrolling but are unsure of the requirements. 

\section*{Logistics}

\textbf{Lectures}: Tuesdays/Thursdays from 9:30am-10:50am -- Location: Harper Memorial 135
\newline\newline \textbf{Disucssion Forum:} We will use the Ed platform as a course discussion board. See the Canvas page for more details.
\newline\newline\textbf{Course Materials}: Lecture materials, assignments and section code will be posted on the course GitHub page at \url{https://github.com/UChicago-pol-methods/plsc-40502-statistical-models/}.

Readings will be listed on the syllabus. I will also post links to any non-textbook readings on the Modules page on Canvas.

\section*{Textbooks} 

The course will involve readings from a variety of different textbook chapters and published papers. The class will not require the purchase of any textbook as they are available online. However, you may wish to obtain a paper copy for your own personal use or reference.

The two primary textbooks from which many readings will be drawn are:

\begin{itemize}
\item Gelman, A., Hill, J., \& Vehtari, A. (2020). \textit{Regression and other stories.} Cambridge University Press. (An introduction to regression and multilevel modeling from an applied perspective) \url{https://avehtari.github.io/ROS-Examples/index.html}
\item Gelman, A., Carlin, J., Stern H., Dunson, D., Vehtari A., \& Rubin, D. (2013). \textit{Bayesian Data Analysis.} 3rd Edition. Chapman and Hall/CRC. (A more advanced text on Bayesian modeling) \url{http://www.stat.columbia.edu/~gelman/book/}
\end{itemize}


\section*{Requirements}

Students’ final grades are based on three components:
\begin{itemize}

\item \textbf{Problem sets} (25\% of the course grade). Students will complete a total of three problem sets throughout the quarter. Problem sets will primarily cover topics from the lecture and section for that week and the previous week.

The goal of the problem sets is to encourage exploration of the material and to provide you with a clear and credible means of assessing your understanding and progress through the course.

Problem sets will be graded on a (+/\checkmark/-) scale with a + awarded for complete and near-perfect work, a \checkmark awarded for generally good work with clear effort shown but with some errors, and a - awarded for significantly incomplete work with major conceptual errors and little effort shown.
 
\begin{itemize}
       \item \textit{Collaboration policy}: I strongly encourage collaboration between students on the problem sets and highly recommend that students discuss problems with each other either in person or via Ed. However, each student is expected to submit their own write-up of the answers and any relevant code. 
        \item \textit{Office hours and online discussion}: Students should feel free to discuss any questions about the problem sets with me during class and during office hours. I also strongly encourage students to post questions about both the problem sets and the readings on the course Ed board and respond to other students’ questions. Responding to other students’ questions will contribute to your participation grade.
        \item \textit{Submission guidelines}: Problem sets will be distributed as \texttt{PDF} and \texttt{Rmarkdown} files (\texttt{.Rmd}). You should submit your answers and any relevant R code in the same format: including an \texttt{Rmarkdown} file (\texttt{.Rmd} extension) and a corresponding rendered \texttt{.pdf} file as your submission. \texttt{Rmarkdown} combines the text formatting syntax of Markdown markup language with the ability to embed and execute chunks of \texttt{R} code directly into a text document. This allows you to present your code, graphical output, and discussion/write-up all in the same document. I highly recommend that you edit the distributed \texttt{Rmarkdown} assignment file for each problem set directly to make organization easier.
        \end{itemize}
\item  \textbf{Take-home midterm and final} (30\% and 35\% of the course grade respectively). The take-home midterm and final exams will have the same format and structure as the problem sets but with one key difference. You are \textbf{not} permitted to collaborate with other students or any other individual on the exams. I will answer any clarifying questions on the \textsc{Ed} discussion board, but will not answer substantive questions.
    \item \textbf{Participation} (10\% of the course grade). I expect students to take an active role in learning in lecture. Engagement with the teaching staff by asking and answering questions will contribute to this grade as will interaction on the Ed discussion board.
\end{itemize}

\section*{Computing}

This course will use the \texttt{R} programming language. This is a free and open source programming language that is available for nearly all computing platforms. You should download and install it from \url{http://www.r-project.org}. Unless you have strong preferences for a specific coding environment, I also highly recommend that you use the free \href{https://rstudio.com}{RStudio} Desktop Integrated Development Environment (IDE) which you can download from \url{https://rstudio.com/products/rstudio/download/#download}. In addition to being a great and simple to use environment for editing code, \texttt{RStudio} makes it very easy to write and compile \texttt{Rmarkdown} documents: the format in which problem sets will be distributed. In addition to base \texttt{R}, we will be frequently using data management and processing tools found in the \href{https://www.tidyverse.org/}{tidyverse} set of packages along with basic graphics and visualization using \href{https://ggplot2.tidyverse.org/}{ggplot2}. 

The course will also introduce the \texttt{Stan} language and software for specifying and estimating Bayesian models. Stan is written in C but has bindings for a variety of programming languages. We will use two interfaces for \texttt{Stan} in R: \texttt{RStan} and \texttt{brms}. 

\subsection*{Policy on Generative Large Language Models}

The rapid growth in both the capabilities and the accessibility of generative large language models (LLMs) such as the GPT series, PaLM, LLaMa, etc... has introduced some novel challenges to the classroom. On the one hand, generative text models can be used as a tool to improve the quality of students' writing. On the other hand, they can be readily used to represent another's work as one's own -- that is, to commit plagiarism. Additionally, LLMs may appear to be useful for some tasks -- such as summarizing a set of texts or finding new sources on a particular topic -- when in fact the outputs are arguably sub-optimal relative to conventional research methods.
\newline\newline
\textbf{My view in short:} Large language models are marvels of \textbf{engineering}. You should use them for \textbf{engineering} tasks, but the task of research is not purely engineering and LLMs are much less effective for the task of doing \textbf{science}.
\newline\newline
By ``engineering," I mean the the iterative task of solving a problem by brainstorming potential solutions, implementing those solutions, and then subsequently \textit{evaluating} the solutions with respect to some clearly defined criteria. The key components here are both the existence of a well-defined problem and the ability to assess whether the proposed solutions are effective.
\newline\newline
Currently, the most obvious and effective use-case for large language models is in coding. I am perfectly happy for you to experiment with using LLMs in debugging code. The interactivity is great for beginning programmers who may have an idea of what they want their code to do, but are unfamiliar with the syntax of a particular language. Likewise, it's an incredibly valuable tool for experienced programmers who want to quickly generate some prototype code that is customized to their particular problem.
\newline\newline
Why is programming an ideal use case? Programming is fundamentally an engineering task. There is a clearly defined problem that a programmer needs to solve via code and there is a straightforward way to evaluate whether a block of code works. As a result, mistakes are easy to catch -- if the code throws an error, something needs to be changed. There is always a human in the loop who is capable of evaluating the output.
\newline\newline
Outside of coding, I do not think LLM outputs are too useful, especially for generating text that is to be submitted without further refinement. In general, you should be cautious about any LLM outputs that you are not able to verify or evaluate yourself. 
\newline\newline
Irrespective of whether LLM outputs are ``good" or not, it is absolutely clear that presenting LLM-generated output as one's own ideas is clearly plagiarism and will be treated as such. This does not rule out all uses of LLM-generated text, but it does rule out most. One use that I would consider acceptable is cleaning up original text that you have written to eliminate grammar mistakes or to rephrase the text to have a clearer style. We already accept the use of spellcheckers and thesauruses that are embedded in most word processors and I don't see this use case as substantively different as long as your original writing is the input. It is important, however, that you are able to evaluate the output and determine that it is conveying exactly what you want to say in exactly the way that you want to say it, just as you would when using any other writing tool.
\newline\newline
Beyond this particular use, \textbf{submitting LLM-generated text as a substitute for your own thinking is not permitted in this class and will be considered plagiarism}. This includes prompting an LLM to compose all or part of your writing and submitting that output either verbatim or with some editing. This policy also applies to generating posts on the Ed discussion board. 
\newline\newline
In general, I do not think that presently there are too many good uses for LLMs for the particular tasks that you will be doing in this class. Although these models can be utilized for things like brainstorming, summarizing text, and search - acting as something of a personalized tutor - and the quality of the model outputs does appear to be steadily growing, I think that you will find significant value to working through the course material directly and asking questions to the teaching staff and to your colleagues in the class.

\section*{Schedule}

A schedule of topics and readings is provided below. Each week will cover a single topic or group of topics. You should treat the readings as a reference and as a more detailed exposition of the topics discussed in lecture. Consult the readings when you want to know more or want a slightly different approach to explaining a particular topic.

\subsection{Week 1: Introduction to Likelihood Inference (January 7)}

\begin{itemize}
  \item What are statistical models good for?
  \item What is a ``parametric" model?
  \item The likelihood function 
  \item Maximum likelihood estimation
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item \textbf{Review:} ``Regression and Other Stories" - Chapters 1-7
\end{itemize}

\textbf{Problem Set 1 Assigned January 7, Due January 20}

\subsection{Week 2: Generalized Linear Models (January 14)}

\begin{itemize}
\item Properties of maximum likelihood estimators
\item Binary outcome models, Event count models, Duration models
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item ``Regression and Other Stories`` - Chapters 13-14
\item Box-Steffensmeier, J. M., \& Jones, B. S. (1997). Time is of the essence: Event history models in political science. \textit{American Journal of Political Science}, 1414-1461.
\item Wooldridge, J. M. (1999). Chapter 8: ``Quasi-likelihood methods for count data." In \textit{Handbook of applied econometrics}, 2, 35-406.
\end{itemize}


\subsection{Week 3: Bayesian Inference (January 21)}

\begin{itemize}
\item Principles of posterior inference
\item How to write a bayesian model
\item Quantities of interest: Posterior Mode, Posterior Mean, Credible Intervals
\item Estimation and inference via Markov Chain Monte Carlo
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item ``Regression and Other Stories``: Chapter 9
\item ``Bayesian Data Analysis`` Chapters 10-11
\end{itemize}


\textbf{Problem Set 2 Assigned January 21, Due February 3}

\subsection{Week 4: Multilevel regression models (January 28)}

\begin{itemize}
\item ``Hierarchical" regression models -- random slopes/random intercept models
\item Estimation via MCMC in \texttt{Stan}
\item Interpreting and analyzing results
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Park, David K., Andrew Gelman, and Joseph Bafumi. "Bayesian multilevel estimation with poststratification: State-level estimates from national polls." Political Analysis 12.4 (2004): 375-385.
\item ``Regression and Other Stories``: Chapter 9, 11, Appendix A
\item ``Bayesian Data Analysis`` Chapter 15
\end{itemize}


\subsection{Week 5: Working with survey data (February 4)}

\begin{itemize}
\item How to approach population inference from non-probability samples: constructing and using weights
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Caughey, D., Berinsky, A. J., Chatfield, S., Hartman, E., Schickler, E., \& Sekhon, J. S. (2020). Target estimation and adjustment weighting for survey nonresponse and sampling bias. Cambridge University Press.
\item Hanretty, Chris. "An introduction to multilevel regression and post-stratification for estimating constituency opinion." Political Studies Review 18.4 (2020): 630-645.
\item Park, David K., Andrew Gelman, and Joseph Bafumi. "Bayesian multilevel estimation with poststratification: State-level estimates from national polls." Political Analysis 12.4 (2004): 375-385.
\end{itemize}

\textbf{Midterm Exam: Assigned February 5, Due February 12}

\subsection{Week 6: Mixture Models and the EM Algorithm (February 11)}

\begin{itemize}
\item Exploratory data analysis and clustering models
\item MLE and MAP estimation via the ``Expectation-Maximization" algorithm
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Imai, Kosuke, and Dustin Tingley. "A statistical method for empirical testing of competing theories." American Journal of Political Science 56.1 (2012): 218-236.
\item McLachlan, Geoffrey J., Sharon X. Lee, and Suren I. Rathnayake. "Finite mixture models." Annual review of statistics and its application 6 (2019): 355-378.
\item "Bayesian Data Analysis" Chapters 13, 22
\end{itemize}

\subsection{Week 7: Item Response Theory and Ideal Point Models (February 18)}

\begin{itemize}
\item Latent variable models from a bayesian perspective
\item ``Ideal point" models for voting
\item Extensions to models of networks 
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Clinton, Joshua, Simon Jackman, and Douglas Rivers. "The statistical analysis of roll call data." American Political Science Review 98, no. 2 (2004): 355-370.
\item Treier, Shawn, and Simon Jackman. "Democracy as a latent variable." American Journal of Political Science 52, no. 1 (2008): 201-217.
\item Martin, Andrew D., and Kevin M. Quinn. "Dynamic ideal point estimation via Markov chain Monte Carlo for the US Supreme Court, 1953–1999." Political analysis 10, no. 2 (2002): 134-153.
\item Burkner, Paul-Christian. "Bayesian item response modeling in R with brms and Stan." arXiv preprint arXiv:1905.09501 (2019).
\end{itemize}

\textbf{Problem Set 3 Assigned February 18, Due March 3}

\subsection{Week 8: Regularization and Model Selection (February 25)}

\begin{itemize}
\item Variable selection and penalized regression (Ridge, LASSO) 
\item Cross-fitting and out-of-sample validation
\end{itemize}

\subsubsection*{Readings}

\begin{itemize}
\item Chapter 3: James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: Springer, 2013.
\item Stanescu, Diana, Erik Wang, and Soichiro Yamauchi. "Using LASSO to assist imputation and predict child well-being." Socius 5 (2019): 2378023118814623.
\end{itemize}

\subsection{Week 9: Flexible Regression}

\begin{itemize}
\item Kernel smoothing and local regression
\item Random forests
\end{itemize}

\begin{itemize}
\item Chapter 6: James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: Springer, 2013.
\item Chapter 15: James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: Springer, 2013.
\end{itemize}

\textbf{Final Exam: Assigned March 5, Due March 14}

\section*{Assignment Schedule}

\begin{itemize}
\item Problem Set 1: Assigned January 7, Due January 20
\item Problem Set 2: Assigned January 21, Due February 3
\item \textbf{Midterm Exam}: Assigned February 5, Due February 12
\item Problem Set 3: Assigned February 18, Due March 3
\item \textbf{Final Exam}: Assigned March 4, Due March 14
\end{itemize}

\section*{Acknowledgments}

This course is indebted to the many wonderful and generous scholars who have developed causal inference curricula in political science departments throughout the world and who have made their course materials available to the public. This course in particular has been heavily inspired by Gov 2001 and Gov 2003 at Harvard University as well as Quant III at MIT. In particular, I thank Matthew Blackwell, Brandon Stewart, Erin Hartman, Molly Roberts, Kosuke Imai, Teppei Yamamoto, Jens Hainmueller, Adam Glynn, Gary King, Justin Grimmer, and In Song Kim whose lecture notes and syllabi have been immensely valuable in the creation of this course. 


\end{document}
 
